{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f77a44",
   "metadata": {},
   "source": [
    "Experiment one (2D poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741a588f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline gamma_opt=1.641822, MSE_int=3.061e-05, MSE_bd=7.161e-06\n",
      "[Baseline TransNet] Interior MSE vs exact: 8.175459e-07\n",
      "\n",
      "Start training residual v_theta ...\n",
      "Epoch     0: total=9.467e-02, pde=2.201e-04, bd=9.445e-03\n",
      "Epoch   500: total=3.181e-05, pde=3.152e-05, bd=2.890e-08\n",
      "Epoch  1000: total=3.096e-05, pde=3.090e-05, bd=5.547e-09\n",
      "Epoch  1500: total=3.075e-05, pde=3.074e-05, bd=1.713e-09\n",
      "Epoch  2000: total=3.068e-05, pde=3.067e-05, bd=8.727e-10\n",
      "Epoch  2500: total=3.065e-05, pde=3.065e-05, bd=7.181e-10\n",
      "\n",
      "[Residual Hybrid Skip] Interior MSE vs exact: 8.163803e-07\n",
      "Baseline MSE: 8.175459e-07  →  Residual Hybrid MSE: 8.163803e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import math, time\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- Problem definition ----------\n",
    "pi = math.pi\n",
    "\n",
    "def u_exact_np(X):\n",
    "    return np.sin(2*pi*X[:,0]) * np.sin(2*pi*X[:,1])\n",
    "\n",
    "def f_forcing_np(X):\n",
    "    return (2*pi)**2 * 2.0 * u_exact_np(X)\n",
    "\n",
    "def u_exact_torch(x):\n",
    "    return torch.sin(2*pi*x[:,0]) * torch.sin(2*pi*x[:,1])\n",
    "\n",
    "def f_forcing_torch(x):\n",
    "    return (2*pi)**2 * 2.0 * torch.sin(2*pi*x[:,0]) * torch.sin(2*pi*x[:,1])\n",
    "\n",
    "# ---------- Sampling ----------\n",
    "def sample_grid_in_box(n_per_dim=50):\n",
    "    xs = np.linspace(-1.0, 1.0, n_per_dim, dtype=np.float64)\n",
    "    X, Y = np.meshgrid(xs, xs)\n",
    "    pts = np.stack([X.ravel(), Y.ravel()], axis=-1)\n",
    "    return pts\n",
    "\n",
    "def sample_boundary_points(n_side=50):\n",
    "    s = np.linspace(-1.0, 1.0, n_side, dtype=np.float64)\n",
    "    pts = []\n",
    "    for y in s: pts.append([-1.0, y])\n",
    "    for y in s: pts.append([1.0, y])\n",
    "    for x in s: pts.append([x, -1.0])\n",
    "    for x in s: pts.append([x,  1.0])\n",
    "    return np.array(pts)\n",
    "\n",
    "# ---------- TransNet helpers ----------\n",
    "def sigma(s):\n",
    "    return np.tanh(s)\n",
    "def sigma_dd(s):\n",
    "    t = np.tanh(s)\n",
    "    sech2 = 1 - t*t\n",
    "    return -2.0 * t * sech2\n",
    "\n",
    "def sample_a_r(M, d=2, R=1.5, seed=1234):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    A = rng.randn(M, d)\n",
    "    norms = np.linalg.norm(A, axis=1, keepdims=True)\n",
    "    A /= norms\n",
    "    r = rng.rand(M) * R\n",
    "    return A, r\n",
    "\n",
    "def build_matrices(X_int, X_bd, A, rvec, gamma):\n",
    "    Xc_int = X_int\n",
    "    Xc_bd = X_bd\n",
    "    s_int = Xc_int.dot(A.T) + rvec.reshape(1,-1)\n",
    "    s_bd = Xc_bd.dot(A.T) + rvec.reshape(1,-1)\n",
    "    S_int = gamma * s_int\n",
    "    S_bd = gamma * s_bd\n",
    "    Psi_int = sigma(S_int)\n",
    "    Psi_bd = sigma(S_bd)\n",
    "    Psi_dd_int = (gamma**2) * sigma_dd(S_int)\n",
    "    F_int = -Psi_dd_int\n",
    "    return F_int, Psi_bd, Psi_int, s_int, s_bd\n",
    "\n",
    "def solve_alpha_ls(F_int, Psi_bd, f_int, g_bd, lambda_L=1.0, lambda_B=1.0, reg=1e-8):\n",
    "    wL = math.sqrt(lambda_L)\n",
    "    wB = math.sqrt(lambda_B)\n",
    "    A_big = np.vstack([wL*F_int, wB*Psi_bd])\n",
    "    rhs = np.concatenate([wL*f_int, wB*g_bd])\n",
    "    AtA = A_big.T @ A_big + reg*np.eye(A_big.shape[1])\n",
    "    Atb = A_big.T @ rhs\n",
    "    alpha = np.linalg.solve(AtA, Atb)\n",
    "    return alpha\n",
    "\n",
    "def u_base_numpy(X, A, rvec, gamma, alpha):\n",
    "    s = X.dot(A.T) + rvec.reshape(1,-1)\n",
    "    S = gamma*s\n",
    "    Psi = sigma(S)\n",
    "    return Psi.dot(alpha)\n",
    "\n",
    "# ========== Step 1: TransNet Baseline ==========\n",
    "M = 300\n",
    "grid_n = 50\n",
    "bd_side = 50\n",
    "X_int = sample_grid_in_box(grid_n)\n",
    "X_bd = sample_boundary_points(bd_side)\n",
    "f_int = f_forcing_np(X_int)\n",
    "g_bd = u_exact_np(X_bd)\n",
    "\n",
    "A, rvec = sample_a_r(M)\n",
    "\n",
    "# Golden search for gamma\n",
    "def eta_of_gamma(gamma):\n",
    "    F_int, Psi_bd, _, _, _ = build_matrices(X_int, X_bd, A, rvec, gamma)\n",
    "    alpha = solve_alpha_ls(F_int, Psi_bd, f_int, g_bd)\n",
    "    resid_int = F_int @ alpha - f_int\n",
    "    resid_bd = Psi_bd @ alpha - g_bd\n",
    "    mse_int = np.mean(resid_int**2)\n",
    "    mse_bd = np.mean(resid_bd**2)\n",
    "    return mse_int + mse_bd, mse_int, mse_bd       #这里后面两个mse是在每给定一组alpha后，计算的pde残差和边界残差\n",
    "\n",
    "def golden_search(func, a, b, tol=1e-3, max_iters=50):\n",
    "    phi = (1 + 5**0.5)/2\n",
    "    invphi = 1/phi\n",
    "    c = b - invphi*(b-a)\n",
    "    d = a + invphi*(b-a)\n",
    "    fc = func(c)\n",
    "    fd = func(d)\n",
    "    for _ in range(max_iters):\n",
    "        if (b - a) < tol: break\n",
    "        if fc[0] < fd[0]:\n",
    "            b = d\n",
    "            d = c\n",
    "            fd = fc\n",
    "            c = b - invphi*(b - a)\n",
    "            fc = func(c)\n",
    "        else:\n",
    "            a = c\n",
    "            c = d\n",
    "            fc = fd\n",
    "            d = a + invphi*(b - a)\n",
    "            fd = func(d)\n",
    "    return (c, fc) if fc[0] < fd[0] else (d, fd)\n",
    "\n",
    "gamma_opt, best = golden_search(eta_of_gamma, 1e-2, 10.0)\n",
    "alpha_opt = solve_alpha_ls(*build_matrices(X_int, X_bd, A, rvec, gamma_opt)[:2], f_int, g_bd)\n",
    "\n",
    "print(f\"Baseline gamma_opt={gamma_opt:.6f}, MSE_int={best[1]:.3e}, MSE_bd={best[2]:.3e}\")\n",
    "\n",
    "# Evaluate baseline error\n",
    "X_test = sample_grid_in_box(100)\n",
    "u_pred_base = u_base_numpy(X_test, A, rvec, gamma_opt, alpha_opt)\n",
    "mse_base = np.mean((u_pred_base - u_exact_np(X_test))**2)\n",
    "print(f\"[Baseline TransNet] Interior MSE vs exact: {mse_base:.6e}\")\n",
    "\n",
    "# ========== Step 2: Residual Hybrid Skip (PyTorch) ==========\n",
    "# 构造 u_base(x) 的 Torch 版本，用于计算 Δu_base 和训练残差方程\n",
    "X_int_torch = torch.tensor(X_int, device=device)\n",
    "X_bd_torch = torch.tensor(X_bd, device=device)\n",
    "A_torch = torch.tensor(A, device=device)\n",
    "rvec_torch = torch.tensor(rvec, device=device)\n",
    "alpha_torch = torch.tensor(alpha_opt, device=device)\n",
    "\n",
    "def u_base_torch(x):\n",
    "    s = x @ A_torch.T + rvec_torch\n",
    "    S = gamma_opt * s\n",
    "    Psi = torch.tanh(S)\n",
    "    return Psi @ alpha_torch\n",
    "\n",
    "def laplacian(u, x):\n",
    "    grads = autograd.grad(u, x, grad_outputs=torch.ones_like(u),\n",
    "                          create_graph=True)[0]\n",
    "    lap = 0\n",
    "    for i in range(x.shape[1]):\n",
    "        grad2 = autograd.grad(grads[:, i], x,\n",
    "                              grad_outputs=torch.ones_like(grads[:, i]),\n",
    "                              create_graph=True)[0][:, i]\n",
    "        lap += grad2\n",
    "    return lap   #这里是在算拉普拉斯算子\n",
    "\n",
    "# 残差网络 vθ\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden=64, depth=4):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(in_dim, hidden), nn.Tanh()]\n",
    "        for _ in range(depth-1):\n",
    "            layers += [nn.Linear(hidden, hidden), nn.Tanh()]\n",
    "        layers += [nn.Linear(hidden, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "model_res = MLP().to(device)\n",
    "opt = torch.optim.Adam(model_res.parameters(), lr=1e-3)\n",
    "lambda_bd = 10.0\n",
    "num_epochs = 3000\n",
    "\n",
    "print(\"\\nStart training residual v_theta ...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    opt.zero_grad()\n",
    "    x_int = X_int_torch\n",
    "    x_int.requires_grad_(True)\n",
    "    v_pred = model_res(x_int)\n",
    "    lap_v = laplacian(v_pred, x_int)\n",
    "    u_base_val = u_base_torch(x_int)\n",
    "    lap_u_base = laplacian(u_base_val, x_int)\n",
    "    r_pde = f_forcing_torch(x_int) + lap_u_base\n",
    "    loss_pde = torch.mean((-lap_v - r_pde)**2)\n",
    "\n",
    "    x_bd = X_bd_torch\n",
    "    v_bd = model_res(x_bd)\n",
    "    loss_bd = torch.mean(v_bd**2)\n",
    "\n",
    "    loss = loss_pde + lambda_bd*loss_bd\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch:5d}: total={loss.item():.3e}, pde={loss_pde.item():.3e}, bd={loss_bd.item():.3e}\")\n",
    "\n",
    "# 评估最终误差\n",
    "X_test_torch = torch.tensor(X_test, device=device)\n",
    "u_pred_final = u_base_torch(X_test_torch) + model_res(X_test_torch)\n",
    "mse_final = torch.mean((u_pred_final - u_exact_torch(X_test_torch))**2).item()\n",
    "\n",
    "print(f\"\\n[Residual Hybrid Skip] Interior MSE vs exact: {mse_final:.6e}\")\n",
    "print(f\"Baseline MSE: {mse_base:.6e}  →  Residual Hybrid MSE: {mse_final:.6e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0242660",
   "metadata": {},
   "source": [
    "Experiment Two (wave equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6bcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching gamma ...\n",
      "gamma_opt = 2.370048, eta = 6.503e-06\n",
      "[Baseline TransNet] MSE vs exact: 1.561160e-04\n",
      "\n",
      "Start training residual v_theta ...\n",
      "Epoch    0: total=4.464e-02, pde=7.177e-05, ic=3.826e-03, per=6.310e-04\n",
      "Epoch  400: total=5.230e-06, pde=4.694e-06, ic=4.907e-08, per=4.509e-09\n",
      "Epoch  800: total=4.395e-06, pde=4.277e-06, ic=1.126e-08, per=5.688e-10\n",
      "Epoch 1200: total=4.207e-06, pde=4.176e-06, ic=2.702e-09, per=3.807e-10\n",
      "Epoch 1600: total=4.173e-06, pde=4.151e-06, ic=1.844e-09, per=3.577e-10\n",
      "Epoch 2000: total=4.549e-06, pde=4.205e-06, ic=3.215e-08, per=2.316e-09\n",
      "Epoch 2400: total=4.165e-06, pde=4.147e-06, ic=1.395e-09, per=4.409e-10\n",
      "Epoch 2800: total=5.717e-06, pde=4.193e-06, ic=1.484e-07, per=4.098e-09\n",
      "\n",
      "[Summary] Baseline MSE = 1.561160e-04 | Residual-Hybrid MSE = 1.550566e-04\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 1: imports & dtype ====\n",
    "import numpy as np\n",
    "import math, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# 双精度对二阶导更稳定\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pi = math.pi\n",
    "\n",
    "\n",
    "# ==== Cell 2: wave equation setup & sampling ====\n",
    "\n",
    "# PDE: u_tt - c u_xx = 0\n",
    "c = 1.0/(16.0*pi*pi)\n",
    "\n",
    "def u_exact_np(X):  # X: (N,2) columns: x,t\n",
    "    x, t = X[:,0], X[:,1]\n",
    "    return 0.5*(np.sin(4*pi*x + t) + np.sin(4*pi*x - t))\n",
    "\n",
    "def u_exact_torch(x):  # x: (N,2) torch\n",
    "    X, T = x[:,0], x[:,1]\n",
    "    return 0.5*(torch.sin(4*pi*X + T) + torch.sin(4*pi*X - T))\n",
    "\n",
    "def IC_u_np(x):   # u(x,0)\n",
    "    return np.sin(4*pi*x)\n",
    "\n",
    "def IC_ut_np(x):  # u_t(x,0)=0\n",
    "    return np.zeros_like(x)\n",
    "\n",
    "# interior: 去掉 t=0 与 x=0/1（这些点用于 IC/BC）\n",
    "def sample_interior(n_x=60, n_t=60):\n",
    "    xs = np.linspace(0.0, 1.0, n_x, dtype=np.float64)\n",
    "    ts = np.linspace(0.0, 2.0, n_t, dtype=np.float64)\n",
    "    X, T = np.meshgrid(xs, ts, indexing='xy')\n",
    "    P = np.stack([X.ravel(), T.ravel()], axis=-1)\n",
    "    mask = (P[:,1] > 0.0) & (P[:,0] > 0.0) & (P[:,0] < 1.0)\n",
    "    return P[mask]\n",
    "\n",
    "def sample_IC(n_x=160):\n",
    "    xs = np.linspace(0.0, 1.0, n_x, dtype=np.float64)\n",
    "    return np.stack([xs, np.zeros_like(xs)], axis=-1)  # t=0\n",
    "\n",
    "def sample_BC_periodic(n_t=160):\n",
    "    ts = np.linspace(0.0, 2.0, n_t, dtype=np.float64)\n",
    "    P0 = np.stack([np.zeros_like(ts), ts], axis=-1) # x=0\n",
    "    P1 = np.stack([np.ones_like(ts),  ts], axis=-1) # x=1\n",
    "    return P0, P1\n",
    "\n",
    "\n",
    "# ==== Cell 3: TransNet basis & linear blocks for wave ====\n",
    "\n",
    "def sigma(s):    return np.tanh(s)\n",
    "def sigma_p(s):  return 1.0 - np.tanh(s)**2\n",
    "def sigma_dd(s): \n",
    "    t = np.tanh(s)\n",
    "    sech2 = 1.0 - t*t\n",
    "    return -2.0*t*sech2\n",
    "\n",
    "def sample_a_r(M, d=2, R=1.3, seed=1234):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    A = rng.randn(M, d)\n",
    "    A /= np.linalg.norm(A, axis=1, keepdims=True)\n",
    "    r = rng.rand(M)*R\n",
    "    return A, r\n",
    "\n",
    "# 构造线性系统各子块：\n",
    "# Lψ = ψ_tt - c ψ_xx = σ''(S)*γ^2*(a_t^2 - c a_x^2)\n",
    "# IC-u: ψ(x,0) ;  IC-ut: ψ_t(x,0) = σ'(S)*γ*a_t\n",
    "# BC-per: ψ(0,t)-ψ(1,t)\n",
    "def build_blocks_wave(X_int, X_IC, X0_t, X1_t, A, r, gamma, xi_c=(0.5,1.0)):    #这个函数是用来构造线性系统的各个子块，即为后面的最小二乘求解做准备\n",
    "    X_int = X_int - np.array(xi_c)[None,:]\n",
    "    X_IC  = X_IC  - np.array(xi_c)[None,:]\n",
    "    X0_t  = X0_t  - np.array(xi_c)[None,:]\n",
    "    X1_t  = X1_t  - np.array(xi_c)[None,:]\n",
    "\n",
    "    S_int = gamma*(X_int @ A.T + r[None,:])\n",
    "    S_IC  = gamma*(X_IC  @ A.T + r[None,:])\n",
    "    S0    = gamma*(X0_t  @ A.T + r[None,:])\n",
    "    S1    = gamma*(X1_t  @ A.T + r[None,:])\n",
    "\n",
    "    ax = A[:,0][None,:]\n",
    "    at = A[:,1][None,:]\n",
    "    L_int  = sigma_dd(S_int)*(gamma**2)*(at**2 - c*ax**2)\n",
    "    Psi_IC = sigma(S_IC)\n",
    "    Psi_tIC = sigma_p(S_IC)*(gamma*at)\n",
    "    BC_per = sigma(S0) - sigma(S1)\n",
    "    return L_int, Psi_IC, Psi_tIC, BC_per\n",
    "\n",
    "\n",
    "def solve_alpha_ls(A_top, A_ic, A_ut, A_per, rhs_top, rhs_ic, rhs_ut, rhs_per,\n",
    "                   w_top=1.0, w_ic=10.0, w_ut=10.0, w_per=10.0, reg=1e-8):\n",
    "    A_big = np.vstack([w_top*A_top, w_ic*A_ic, w_ut*A_ut, w_per*A_per])\n",
    "    b_big = np.concatenate([w_top*rhs_top, w_ic*rhs_ic, w_ut*rhs_ut, w_per*rhs_per])\n",
    "    AtA = A_big.T @ A_big + reg*np.eye(A_big.shape[1])\n",
    "    Atb = A_big.T @ b_big\n",
    "    alpha = np.linalg.solve(AtA, Atb)\n",
    "    return alpha\n",
    "\n",
    "def u_base_numpy(X, A, r, gamma, alpha, xi_c=(0.5,1.0)):\n",
    "    X_c = X - np.array(xi_c)[None,:]\n",
    "    S = gamma*(X_c @ A.T + r[None,:])\n",
    "    return sigma(S) @ alpha\n",
    "\n",
    "\n",
    "# ==== Cell 4: golden search for gamma & baseline ====\n",
    "\n",
    "def golden_search(func, a, b, tol=1e-3, max_iters=50):\n",
    "    phi = (1 + 5**0.5)/2.0\n",
    "    invphi = 1.0/phi\n",
    "    c = b - invphi*(b-a)\n",
    "    d = a + invphi*(b-a)\n",
    "    fc = func(c)\n",
    "    fd = func(d)\n",
    "    it = 0\n",
    "    while (b-a)>tol and it<max_iters:\n",
    "        it += 1\n",
    "        if fc[0] < fd[0]:\n",
    "            b, d, fd = d, c, fc\n",
    "            c = b - invphi*(b-a)\n",
    "            fc = func(c)\n",
    "        else:\n",
    "            a, c, fc = c, d, fd\n",
    "            d = a + invphi*(b-a)\n",
    "            fd = func(d)\n",
    "    return (c, fc) if fc[0] < fd[0] else (d, fd)\n",
    "\n",
    "# --- 数据采样 ---\n",
    "X_int = sample_interior(60, 60)       # interior\n",
    "X_IC  = sample_IC(160)                # t=0\n",
    "X0_t, X1_t = sample_BC_periodic(160)  # x=0, x=1\n",
    "\n",
    "rhs_int = np.zeros(X_int.shape[0])        # L(u)=0\n",
    "rhs_IC  = IC_u_np(X_IC[:,0])              # u(x,0)\n",
    "rhs_ut  = IC_ut_np(X_IC[:,0])             # u_t(x,0)=0\n",
    "rhs_per = np.zeros(X0_t.shape[0])         # periodic: u(0,t)-u(1,t)=0\n",
    "\n",
    "# --- 随机基 ---\n",
    "M = 300\n",
    "seed = 2025\n",
    "np.random.seed(seed)\n",
    "A, r = sample_a_r(M, d=2, R=1.3, seed=seed+1)\n",
    "\n",
    "# --- γ 搜索目标 ---\n",
    "def eta_of_gamma(gamma):\n",
    "    L_int, Psi_IC, Psi_tIC, BC_per = build_blocks_wave(X_int, X_IC, X0_t, X1_t, A, r, gamma)\n",
    "    alpha = solve_alpha_ls(L_int, Psi_IC, Psi_tIC, BC_per, rhs_int, rhs_IC, rhs_ut, rhs_per,\n",
    "                          w_top=1.0, w_ic=10.0, w_ut=10.0, w_per=10.0)\n",
    "    e_int = L_int@alpha - rhs_int\n",
    "    e_ic  = Psi_IC@alpha - rhs_IC\n",
    "    e_ut  = Psi_tIC@alpha - rhs_ut\n",
    "    e_per = BC_per@alpha - rhs_per\n",
    "    mse = (np.mean(e_int**2) + 10.0*np.mean(e_ic**2) + 10.0*np.mean(e_ut**2) + 10.0*np.mean(e_per**2))\n",
    "    return (mse,)\n",
    "\n",
    "print(\"Searching gamma ...\")\n",
    "gamma_opt, best = golden_search(eta_of_gamma, 1e-2, 10.0, tol=1e-3, max_iters=50)\n",
    "print(f\"gamma_opt = {gamma_opt:.6f}, eta = {best[0]:.3e}\")\n",
    "\n",
    "# --- 最终 α & 基线误差 ---\n",
    "L_int, Psi_IC, Psi_tIC, BC_per = build_blocks_wave(X_int, X_IC, X0_t, X1_t, A, r, gamma_opt)\n",
    "alpha_opt = solve_alpha_ls(L_int, Psi_IC, Psi_tIC, BC_per, rhs_int, rhs_IC, rhs_ut, rhs_per,\n",
    "                           w_top=1.0, w_ic=10.0, w_ut=10.0, w_per=10.0)\n",
    "\n",
    "X_test = np.stack(np.meshgrid(np.linspace(0,1,120), np.linspace(0,2,120), indexing='xy'), axis=-1).reshape(-1,2)\n",
    "u_base = u_base_numpy(X_test, A, r, gamma_opt, alpha_opt)\n",
    "mse_base = np.mean((u_base - u_exact_np(X_test))**2)\n",
    "print(f\"[Baseline TransNet] MSE vs exact: {mse_base:.6e}\")\n",
    "\n",
    "\n",
    "\n",
    "# ==== Cell 5: Torch u_base / operator / r_PDE precompute ====\n",
    "\n",
    "# 常量参数转 torch（不需要 requires_grad）\n",
    "A_t  = torch.tensor(A, device=device)\n",
    "r_t  = torch.tensor(r, device=device)\n",
    "alpha_t = torch.tensor(alpha_opt, device=device)\n",
    "gamma_t = torch.tensor(gamma_opt, device=device)\n",
    "\n",
    "def u_base_torch(x, xi_c=(0.5,1.0)):\n",
    "    xi_c_t = torch.tensor(xi_c, device=device)\n",
    "    x_c = x - xi_c_t[None,:]\n",
    "    S = gamma_t*(x_c @ A_t.T + r_t[None,:])\n",
    "    return torch.tanh(S) @ alpha_t\n",
    "\n",
    "\n",
    "# 波动方程算子 L(u) = u_tt - c u_xx\n",
    "def wave_operator(u, x):     #这里因为是二阶导，所以要做两遍autograd\n",
    "    grad = autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    du_dx = grad[:,0]\n",
    "    du_dt = grad[:,1]\n",
    "    d2u_dxx = autograd.grad(du_dx, x, grad_outputs=torch.ones_like(du_dx), create_graph=True)[0][:,0]\n",
    "    d2u_dtt = autograd.grad(du_dt, x, grad_outputs=torch.ones_like(du_dt), create_graph=True)[0][:,1]\n",
    "    return d2u_dtt - c*d2u_dxx\n",
    "\n",
    "# 预计算 r_PDE = -L(u_base) ；不要在 no_grad 中做二阶导\n",
    "def precompute_r_pde(X, chunk=8192):\n",
    "    outs = []\n",
    "    N = X.shape[0]\n",
    "    for i in range(0, N, chunk):\n",
    "        xb = X[i:i+chunk].clone().detach().requires_grad_(True)\n",
    "        ub = u_base_torch(xb)\n",
    "        Lub = wave_operator(ub, xb)\n",
    "        outs.append((-Lub).detach())   # 结果 detach 成常量监督\n",
    "    return torch.cat(outs, dim=0)\n",
    "\n",
    "# 训练集（torch）\n",
    "Xin_t  = torch.tensor(X_int, device=device)\n",
    "Xic_t  = torch.tensor(X_IC,  device=device)\n",
    "X0_t_t = torch.tensor(X0_t,  device=device)\n",
    "X1_t_t = torch.tensor(X1_t,  device=device)\n",
    "\n",
    "# 预计算 r_PDE\n",
    "r_int = precompute_r_pde(Xin_t, chunk=8192)\n",
    "\n",
    "\n",
    "# ==== Cell 6: Residual network v_theta & training ====\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden=64, depth=4):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(in_dim, hidden), nn.Tanh()]\n",
    "        for _ in range(depth-1):\n",
    "            layers += [nn.Linear(hidden, hidden), nn.Tanh()]\n",
    "        layers += [nn.Linear(hidden, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "model = MLP().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "lambda_ic  = 10.0\n",
    "lambda_per = 10.0\n",
    "epochs = 3000\n",
    "\n",
    "print(\"\\nStart training residual v_theta ...\")\n",
    "\n",
    "for ep in range(epochs):    #这个循环是在做MLP的训练\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # PDE 残差（域内）：L(v) ≈ r_PDE\n",
    "    x = Xin_t.clone().detach().requires_grad_(True)\n",
    "    v = model(x)\n",
    "    L_v = wave_operator(v, x)\n",
    "    loss_pde = torch.mean((L_v - r_int)**2)\n",
    "\n",
    "    # 初值：v(x,0)=0, v_t(x,0)=0\n",
    "    v_ic = model(Xic_t)                     # 值为零\n",
    "    loss_ic_val = torch.mean(v_ic**2)\n",
    "\n",
    "    Xic_req = Xic_t.clone().detach().requires_grad_(True)\n",
    "    v_ic2 = model(Xic_req)\n",
    "    grad_ic = autograd.grad(v_ic2, Xic_req, grad_outputs=torch.ones_like(v_ic2), create_graph=True)[0]\n",
    "    vt_ic = grad_ic[:,1]                    # ∂v/∂t at t=0\n",
    "    loss_ic_vel = torch.mean(vt_ic**2)\n",
    "    loss_ic = loss_ic_val + loss_ic_vel\n",
    "\n",
    "    # 周期边界：v(0,t)-v(1,t)=0\n",
    "    v0 = model(X0_t_t)\n",
    "    v1 = model(X1_t_t)\n",
    "    loss_per = torch.mean((v0 - v1)**2)\n",
    "\n",
    "    loss = loss_pde + lambda_ic*loss_ic + lambda_per*loss_per\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if ep % 400 == 0:\n",
    "        print(f\"Epoch {ep:4d}: total={loss.item():.3e}, pde={loss_pde.item():.3e}, ic={loss_ic.item():.3e}, per={loss_per.item():.3e}\")\n",
    "\n",
    "\n",
    "# ==== Cell 7: evaluation ====\n",
    "with torch.no_grad():\n",
    "    Xt = torch.tensor(X_test, device=device)\n",
    "    u_pred = u_base_torch(Xt) + model(Xt)\n",
    "    mse_final = torch.mean((u_pred - u_exact_torch(Xt))**2).item()\n",
    "\n",
    "print(f\"\\n[Summary] Baseline MSE = {mse_base:.6e} | Residual-Hybrid MSE = {mse_final:.6e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
